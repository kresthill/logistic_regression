# Diabetes Prediction using Logistic Regression

A complete end-to-end machine learning pipeline for predicting diabetes using the Pima Indian Diabetes dataset. This project demonstrates best practices in data preprocessing, model training, evaluation, and optimization.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![scikit-learn](https://img.shields.io/badge/scikit--learn-1.0+-orange.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

## üìã Table of Contents

- [Overview](#overview)
- [Dataset](#dataset)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Methodology](#methodology)
- [Results](#results)
- [Future Improvements](#future-improvements)
- [Contributing](#contributing)
- [License](#license)

## üéØ Overview

This project builds a Logistic Regression classifier to predict diabetes onset based on diagnostic measurements. The pipeline includes:

- Intelligent data cleaning with domain-specific preprocessing
- Feature engineering and imputation
- L2 regularization optimization
- Threshold tuning for improved performance
- Comprehensive model evaluation and interpretation

**Final Model Performance: 77.92% accuracy**

## üìä Dataset

**Source:** Pima Indian Diabetes Database

**Description:** This dataset contains diagnostic measurements from female patients of Pima Indian heritage, aged 21 years or older.

**Features:**
- `Pregnancies`: Number of times pregnant
- `Glucose`: Plasma glucose concentration (2 hours in oral glucose tolerance test)
- `BloodPressure`: Diastolic blood pressure (mm Hg)
- `SkinThickness`: Triceps skin fold thickness (mm)
- `Insulin`: 2-Hour serum insulin (mu U/ml)
- `BMI`: Body mass index (weight in kg/(height in m)^2)
- `DiabetesPedigreeFunction`: Diabetes pedigree function (genetic risk score)
- `Age`: Age in years
- `Outcome`: Class variable (0: no diabetes, 1: diabetes)

**Dataset Statistics:**
- Total samples: 768
- Positive cases (diabetes): 268 (34.9%)
- Negative cases (no diabetes): 500 (65.1%)

## ‚ú® Features

- **Smart Data Preprocessing**: Identifies and handles biologically impossible zero values
- **Median Imputation**: Replaces missing values with feature-specific medians
- **Regularization Tuning**: Optimizes L2 penalty across 200 candidate values
- **Threshold Optimization**: Finds optimal decision threshold beyond default 0.5
- **Comprehensive Evaluation**: Confusion matrices, classification reports, and coefficient analysis
- **Visualizations**: FNR vs FPR curves for threshold analysis

## üî¨ Methodology

### 1. Data Cleaning & Preprocessing

**Problem Identified:** Several features contained biologically impossible zero values:

| Feature | Zero Counts | % of Data | Action |
|---------|-------------|-----------|--------|
| Glucose | 5 | 0.65% | Replace with NaN ‚Üí Median imputation |
| BloodPressure | 35 | 4.56% | Replace with NaN ‚Üí Median imputation |
| SkinThickness | 227 | 29.56% | Replace with NaN ‚Üí Median imputation |
| Insulin | 374 | 48.70% | Replace with NaN ‚Üí Median imputation |
| BMI | 11 | 1.43% | Replace with NaN ‚Üí Median imputation |

**Rationale:** Features like Pregnancies, Age, and Pedigree can naturally be zero, so they were excluded from imputation.

### 2. Train/Test Split

```python
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, shuffle=True, random_state=42
)
```

- 80% training data (614 samples)
- 20% test data (154 samples)
- Random state ensures reproducibility

### 3. Regularization Tuning (L2/Ridge)

Evaluated 200 logarithmically-spaced values of C (inverse regularization strength):

- **Best C:** 0.000512462311557789
- **Test Accuracy (with regularization):** 76.62%
- **Test Accuracy (without regularization):** 75.32%

**Impact:** +1.3% accuracy improvement through regularization

### 4. Threshold Optimization

Instead of default 0.5 threshold, evaluated thresholds from 0 to 1:

- **Optimal Threshold:** 0.520
- **Minimum Total Error (FNR + FPR):** 0.5212
- **Final Accuracy:** 77.92%

## üìà Results

### Model Performance

| Metric | Value |
|--------|-------|
| **Accuracy** | 77.92% |
| **Precision (Class 0)** | 0.80 |
| **Recall (Class 0)** | 0.83 |
| **F1-Score (Class 0)** | 0.81 |
| **Precision (Class 1)** | 0.67 |
| **Recall (Class 1)** | 0.62 |
| **F1-Score (Class 1)** | 0.64 |

### Confusion Matrix (Optimized Threshold)

```
                Predicted
              No Diabetes  Diabetes
Actual   
No Diabetes      87          12
Diabetes         22          33
```

### Feature Importance (Coefficients)

| Feature | Coefficient | Interpretation |
|---------|-------------|----------------|
| DiabetesPedigreeFunction | 0.55 | Strongest positive predictor (genetic risk) |
| BMI | 0.10 | Higher BMI increases diabetes risk |
| Pregnancies | 0.067 | More pregnancies ‚Üí higher risk |
| Glucose | 0.037 | Higher glucose indicates diabetes |
| Age | 0.034 | Older age increases risk |
| SkinThickness | 0.007 | Slight positive effect |
| Insulin | -0.001 | Weak negative (noisy data) |
| BloodPressure | -0.013 | Slight negative effect |

**Medical Validity:** Model coefficients align with established medical understanding of diabetes risk factors.

### Key Insights

‚úÖ **No Overfitting:** Training accuracy (76.87%) ‚âà Test accuracy (77.92%)

‚úÖ **Interpretability:** Linear model provides clear feature importance

‚úÖ **Regularization Benefit:** L2 regularization improved generalization

‚úÖ **Threshold Tuning:** Moving threshold from 0.5 ‚Üí 0.52 improved accuracy by 2.6%

‚ö†Ô∏è **Class Imbalance:** Model performs better on majority class (no diabetes)

## üîÆ Future Improvements

### 1. Address Class Imbalance
- Implement SMOTE (Synthetic Minority Over-sampling)
- Use class weights in model training
- Try ensemble methods with balanced sampling

### 2. Advanced Models
- **Random Forest:** Handle non-linear relationships
- **XGBoost:** Gradient boosting for better performance
- **Neural Networks:** Deep learning for complex patterns

### 3. Enhanced Validation
- **K-Fold Cross-Validation:** More robust performance estimates
- **Stratified Sampling:** Maintain class distribution across folds
- **ROC Curve & AUC:** Better evaluation of classifier performance

### 4. Hyperparameter Optimization
- **GridSearchCV:** Exhaustive search over parameter grid
- **RandomizedSearchCV:** Efficient random sampling
- **Bayesian Optimization:** Smart parameter search

### 5. Feature Engineering
- Polynomial features for interaction effects
- Feature scaling normalization techniques
- Domain-specific feature creation

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üë• Authors

- **Your Name** - Kresthill Festus *Initial work* - [YourGitHub](https://github.com/kresthill)

## üôè Acknowledgments

- Pima Indian Diabetes Database from the National Institute of Diabetes and Digestive and Kidney Diseases
- scikit-learn documentation and community
- Medical domain experts for validation of feature importance

## üìß Contact

For questions or feedback, please contact:
- Email: your.email@example.com
- GitHub: [@yourusername](https://github.com/yourusername)
- LinkedIn: [Your Name](https://linkedin.com/in/yourprofile)

---

**‚≠ê If you find this project helpful, please consider giving it a star!**